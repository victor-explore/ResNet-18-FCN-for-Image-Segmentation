{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","mount_file_id":"11yXC6noorqkHPJAaUFy3SRsijJGEJhaL","authorship_tag":"ABX9TyMDxqaarStQiu0gXwCh2KwL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Prepare data**"],"metadata":{"id":"y6WMUp7rHbSW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rkfZPu9gqIts","executionInfo":{"status":"ok","timestamp":1708672445004,"user_tz":-330,"elapsed":15742,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"28328846-a773-4f22-f5b1-2bb88ead5709"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**Load data**"],"metadata":{"id":"Qt0EDKIauVx8"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","import json\n","from torch.utils.data import Dataset\n","\n","class SegmentationDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, colormap_file, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        with open(colormap_file) as f:\n","            self.colormap = json.load(f)\n","            self.color2label = {tuple(color): idx for idx, color in enumerate(self.colormap.values())}\n","        self.images = os.listdir(image_dir)\n","        print(f\"Found {len(self.images)} images.\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.images[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png'))  # Adjust if necessary\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"RGB\")\n","        mask = self.rgb_to_mask(mask)\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            mask = torch.from_numpy(mask).long()\n","\n","        # Debug statements\n","        print(f\"Loading image: {img_name}\")\n","        print(f\"Image shape: {image.shape}\")\n","        print(f\"Mask unique values: {torch.unique(mask)}\")\n","\n","        return image, mask\n","\n","    def rgb_to_mask(self, mask):\n","        \"\"\"Convert a RGB mask to a class map mask.\"\"\"\n","        mask = np.array(mask)\n","        class_map = np.zeros(mask.shape[:2], dtype=np.int32)\n","\n","        for rgb, class_id in self.color2label.items():\n","            equality = np.equal(mask, rgb)\n","            class_map[np.all(equality, axis=-1)] = class_id\n","\n","        # Debug statement\n","        print(f\"Unique classes in mask: {np.unique(class_map)}\")\n","\n","        return class_map\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","image_dir = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/images'\n","mask_dir = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/masks'\n","colormap_file = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/label2cmap.json'\n","\n","# Create the dataset\n","dataset = SegmentationDataset(image_dir, mask_dir, colormap_file, transform=transform)\n","\n","image, mask = dataset[0]  # inspect a sample"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnLaF_1CqZx1","executionInfo":{"status":"ok","timestamp":1708672452890,"user_tz":-330,"elapsed":7890,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"393ae275-9ba0-42f9-a16c-34828ed00fa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 298 images.\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (185).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n"]}]},{"cell_type":"markdown","source":["**Split training and testing data**"],"metadata":{"id":"G74Z9U-juZXU"}},{"cell_type":"code","source":["\n","split_file_path = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/train_test_split.json'\n","\n","# Read the JSON file\n","with open(split_file_path, 'r') as file:\n","    split_data = json.load(file)\n","\n","train_images = split_data['train']\n","test_images = split_data['test']\n","\n","print(f\"Total training images: {len(train_images)}\")\n","print(f\"Total testing images: {len(test_images)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FES0aD7evIFF","executionInfo":{"status":"ok","timestamp":1708672452891,"user_tz":-330,"elapsed":24,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"a99bf62e-36b4-45e1-8fe1-4d90739bbbfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training images: 248\n","Total testing images: 50\n"]}]},{"cell_type":"code","source":["class SegmentationDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, colormap_file, subset, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        with open(colormap_file) as f:\n","            self.colormap = json.load(f)\n","            self.color2label = {tuple(color): idx for idx, color in enumerate(self.colormap.values())}\n","        # Use only the subset of images specified ('train' or 'test')\n","        if subset == 'train':\n","            self.images = [img for img in os.listdir(image_dir) if img in train_images]\n","        elif subset == 'test':\n","            self.images = [img for img in os.listdir(image_dir) if img in test_images]\n","        else:\n","            raise ValueError(\"Subset must be either 'train' or 'test'\")\n","        print(f\"Found {len(self.images)} {subset} images.\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.images[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png'))  # Adjust if necessary\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"RGB\")\n","        mask = self.rgb_to_mask(mask)\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            mask = torch.from_numpy(mask).long()\n","\n","        # Debug statements\n","        print(f\"Loading image: {img_name}\")\n","        print(f\"Image shape: {image.shape}\")\n","        print(f\"Mask unique values: {torch.unique(mask)}\")\n","\n","        return image, mask\n","\n","    def rgb_to_mask(self, mask):\n","        \"\"\"Convert a RGB mask to a class map mask.\"\"\"\n","        mask = np.array(mask)\n","        class_map = np.zeros(mask.shape[:2], dtype=np.int32)\n","\n","        for rgb, class_id in self.color2label.items():\n","            equality = np.equal(mask, rgb)\n","            class_map[np.all(equality, axis=-1)] = class_id\n","\n","        # Debug statement\n","        print(f\"Unique classes in mask: {np.unique(class_map)}\")\n","\n","        return class_map"],"metadata":{"id":"RSoQCM8vvnAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the training dataset\n","train_dataset = SegmentationDataset(image_dir, mask_dir, colormap_file, 'train', transform=transform)\n","\n","# Create the testing dataset\n","test_dataset = SegmentationDataset(image_dir, mask_dir, colormap_file, 'test', transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA2Qhkgev7yK","executionInfo":{"status":"ok","timestamp":1708672452892,"user_tz":-330,"elapsed":20,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"fcad9a7a-f36e-44e6-8950-e832e0967d8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 248 train images.\n","Found 50 test images.\n"]}]},{"cell_type":"markdown","source":["**Data loaders**"],"metadata":{"id":"ttEwflokwHgs"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# Set the batch size for the data loaders\n","batch_size = 16\n","\n","# Create the training data loader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# Create the testing data loader\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","# Verify the data loaders\n","print(f\"Total train batches: {len(train_loader)}\")\n","print(f\"Total test batches: {len(test_loader)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfZT2SFIwJmX","executionInfo":{"status":"ok","timestamp":1708672452892,"user_tz":-330,"elapsed":13,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"302c522b-328a-4dbc-abc2-f969c5b331b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total train batches: 16\n","Total test batches: 4\n"]}]},{"cell_type":"markdown","source":["**Model - without skip connections**"],"metadata":{"id":"3pS3lXLE0cgv"}},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","from torch.nn import functional as F\n","\n","# Load the pre-trained ResNet-18 model\n","resnet18 = models.resnet18(pretrained=True)\n","\n","# Freeze the parameters (weights) of the ResNet-18 backbone\n","for param in resnet18.parameters():\n","    param.requires_grad = False\n","\n","# Remove the average pooling and fully connected layers\n","modules = list(resnet18.children())[:-2]\n","resnet18_backbone = torch.nn.Sequential(*modules)\n","\n","# Define the number of classes for the segmentation task\n","num_classes = 9  # Change this to your actual number of classes\n","\n","# Add intermediate upsampling layers\n","upsample_layers = [\n","    torch.nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=0, bias=False),\n","    torch.nn.BatchNorm2d(256),\n","    torch.nn.ReLU(inplace=True),\n","    torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","    torch.nn.BatchNorm2d(256),\n","    torch.nn.ReLU(inplace=True),\n","    torch.nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=0, bias=False),\n","    torch.nn.BatchNorm2d(128),\n","    torch.nn.ReLU(inplace=True),\n","    torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n","    torch.nn.BatchNorm2d(128),\n","    torch.nn.ReLU(inplace=True),\n","    torch.nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2, padding=0, bias=False),\n","    torch.nn.BatchNorm2d(64),\n","    torch.nn.ReLU(inplace=True),\n","    torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n","    torch.nn.BatchNorm2d(64),\n","    torch.nn.ReLU(inplace=True),\n","    torch.nn.Conv2d(64, num_classes, kernel_size=1)  # Final layer to get to the number of classes\n","]\n","\n","# Create the full model by combining the backbone and upsampling layers\n","resnet18_segmentation = torch.nn.Sequential(*resnet18_backbone, *upsample_layers)\n","\n","# Initialize the weights of the upsampling layers\n","def init_weights(m):\n","    if isinstance(m, torch.nn.ConvTranspose2d) or isinstance(m, torch.nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            torch.nn.init.zeros_(m.bias)\n","\n","# Apply the weights initialization to only the upsampling layers\n","for m in upsample_layers:\n","    m.apply(init_weights)\n","\n","# Print the modified model to verify the changes\n","print(resnet18_segmentation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKnC0mTs09ZH","executionInfo":{"status":"ok","timestamp":1708673587596,"user_tz":-330,"elapsed":4,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"bd5a267b-968a-46e6-fafe-2ce4b559a448"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (2): ReLU(inplace=True)\n","  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (5): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (6): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (7): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (8): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n","  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (10): ReLU(inplace=True)\n","  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (13): ReLU(inplace=True)\n","  (14): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n","  (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (16): ReLU(inplace=True)\n","  (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (19): ReLU(inplace=True)\n","  (20): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n","  (21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (22): ReLU(inplace=True)\n","  (23): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (24): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (25): ReLU(inplace=True)\n","  (26): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",")\n"]}]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"TMF_vmIP4qkw"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Set up the loss function\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Set up the optimizer, only optimizing the parameters that require gradients\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet18_segmentation.parameters()), lr=0.001)"],"metadata":{"id":"N1e-ppu74taa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, train_loader, num_epochs=25):\n","    model.train()  # Set model to training mode\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        for images, masks in train_loader:\n","            # Move tensors to the appropriate device\n","            images, masks = images.to(device), masks.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, masks)\n","\n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n","\n","    print('Training complete')\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","resnet18_segmentation = resnet18_segmentation.to(device)"],"metadata":{"id":"bbqApL5B4wxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.functional import interpolate\n","\n","def train_model(model, criterion, optimizer, train_loader, num_epochs=1):\n","    model.train()  # Set model to training mode\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        for images, masks in train_loader:\n","            images, masks = images.to(device), masks.to(device)\n","\n","\n","            masks_resized = interpolate(masks.unsqueeze(1).float(),\n","                                        size=(272, 480),\n","                                        mode='nearest').squeeze(1).long()\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, masks_resized)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n","\n","    print('Training complete')\n","\n","train_model(resnet18_segmentation, criterion, optimizer, train_loader, num_epochs=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tvUhCvH5unV","executionInfo":{"status":"ok","timestamp":1708674811469,"user_tz":-330,"elapsed":203747,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"39798fb1-1b0d-4ba5-9ae1-63d5153d73fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 5 6]\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (331).pngLoading image: 2022-08-24 (53).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Image shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([0, 1, 5, 6])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 8]\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (348).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (209).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 8])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (216).png\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Loading image: 2022-08-24 (71).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (292).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (220).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (193).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (47).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6]Unique classes in mask: [0 1 2 3 4 5 7 8]\n","\n","Loading image: 2022-08-24 (173).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (83).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Unique classes in mask: [0 1 2 3 4 5 7 8]Loading image: 2022-08-24 (185).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (130).png\n","Mask unique values: tensor([0, 1, 5, 6, 7])Image shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (92).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (163).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (99).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (339).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (107).png\n","Image shape: torch.Size([3, 1080, 1920])Loading image: 2022-08-24 (308).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 3 7 8]\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (32).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (62).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 7, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Unique classes in mask: [0 1 6]Loading image: 2022-08-24 (100).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Loading image: 2022-08-24 (147).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 3 4 6 8]\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (275).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (144).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 6, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (93).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (199).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (284).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Loading image: 2022-08-24 (351) - Copy.png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (293).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (232).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (38).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (259).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (312).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (64).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (316).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6]\n","Loading image: 2022-08-24 (341).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (24).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (26).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (19).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (49).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (168).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (258).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (260).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (298).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (125).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Loading image: 2022-08-24 (98).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 3 4 6 8]\n","Loading image: 2022-08-24 (282).png\n","Image shape: torch.Size([3, 1080, 1920])Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","\n","Loading image: 2022-08-24 (7).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 8])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (221).pngLoading image: 2022-08-24 (159).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Image shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([0, 1, 6, 7])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 5 6 8]\n","Unique classes in mask: [1 2 3 4 5 6 7 8]Loading image: 2022-08-24 (326).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (23).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 3 7 8]\n","Loading image: 2022-08-24 (344).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 7, 8])Unique classes in mask: [0 1 2 3 5 6 7 8]\n","\n","Loading image: 2022-08-24 (87).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 3 4 5 6]\n","Loading image: 2022-08-24 (236).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6])\n","Loading image: 2022-08-24 (5).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (269).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (108).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (186).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7]\n","Loading image: 2022-08-24 (191).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (175).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (63).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (303).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7]\n","Loading image: 2022-08-24 (106).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (304).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (291).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 6]\n","Loading image: 2022-08-24 (88).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (181).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Loading image: 2022-08-24 (279).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (170).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1]\n","Loading image: 2022-08-24 (78).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1])\n","Unique classes in mask: [0 1 5 6 8]\n","Loading image: 2022-08-24 (114).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (105).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 6 8]\n","Loading image: 2022-08-24 (148).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (294).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 6]\n","Loading image: 2022-08-24 (121).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (127).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 5 7 8]\n","Loading image: 2022-08-24 (117).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (131).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (346).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (342).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (155).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (153).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (238).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (133).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (223).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (137).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (56).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (162).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 3 4 5 8]\n","Loading image: 2022-08-24 (48).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (337).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 6 7]\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Loading image: 2022-08-24 (177).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 6 7]\n","Loading image: 2022-08-24 (325).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 2 3 4 5 8]\n","Mask unique values: tensor([0, 1, 2, 3, 6, 7])\n","Loading image: 2022-08-24 (178).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (40).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 3 5 6 7]\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Loading image: 2022-08-24 (118).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (132).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 5 6 7]\n","Mask unique values: tensor([0, 1, 6, 7])\n","Loading image: 2022-08-24 (290).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (207).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 3 5 6 7 8]\n","Loading image: 2022-08-24 (166).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 6]\n","Loading image: 2022-08-24 (120).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6])\n","Unique classes in mask: [0 1 2 3 4 5]\n","Loading image: 2022-08-24 (176).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (311).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Loading image: 2022-08-24 (300).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (36).pngUnique classes in mask: [0 1 2 3 4 5 7 8]\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (152).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [0 1 2 3 6 7 8]\n","Loading image: 2022-08-24 (242).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (30).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])Mask unique values: tensor([0, 1, 2, 3, 6, 7, 8])\n","\n","Unique classes in mask: [0 1 3 5 6 7]\n","Loading image: 2022-08-24 (187).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 7])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (277).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (28).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (228).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (335).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (34).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (21).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 8]\n","Loading image: 2022-08-24 (84).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (210).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (22).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 3 4 8]\n","Loading image: 2022-08-24 (13).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (229).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (323).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (126).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (206).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (332).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Loading image: 2022-08-24 (281).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (295).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (149).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (74).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (222).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (171).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (89).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 5 6]\n","Loading image: 2022-08-24 (340).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6])\n","Unique classes in mask: [0 1 4 6 7 8]\n","Loading image: 2022-08-24 (14).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 4, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (154).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 2 3 8]\n","Loading image: 2022-08-24 (128).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (104).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 2 3 5 6 7 8]\n","Loading image: 2022-08-24 (211).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (244).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (35).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (306).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 5 6 8]\n","Loading image: 2022-08-24 (119).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (54).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (330).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (158).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (61).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (270).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (136).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (72).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (85).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (343).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (257).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (350).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7]\n","Loading image: 2022-08-24 (333).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n","Loading image: 2022-08-24 (196).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 3 4 5 6 8]\n","Loading image: 2022-08-24 (67).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (230).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (208).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (273).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (43).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6]\n","Loading image: 2022-08-24 (164).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (241).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 8]\n","Loading image: 2022-08-24 (231).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (267).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (203).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (102).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (129).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (82).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (214).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (55).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (59).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (57).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (288).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 5 7 8]\n","Loading image: 2022-08-24 (265).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (283).png\n","\n","Image shape: torch.Size([3, 1080, 1920])Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (200).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (42).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (213).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (112).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (347).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 6]\n","Loading image: 2022-08-24 (165).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (225).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (349).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (243).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (66).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (101).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 4 5 6 8]\n","Loading image: 2022-08-24 (90).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 4, 5, 6, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (157).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (161).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (219).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (334).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (41).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (73).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (299).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (172).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (97).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (297).png\n","Image shape: torch.Size([3, 1080, 1920])Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 4 5 6]\n","Loading image: 2022-08-24 (276).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (134).pngMask unique values: tensor([1, 4, 5, 6])\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (287).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Loading image: 2022-08-24 (226).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (182).png\n","Image shape: torch.Size([3, 1080, 1920])Unique classes in mask: [0 1 6 7]\n","\n","Loading image: 2022-08-24 (50).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (296).png\n","Image shape: torch.Size([3, 1080, 1920])Loading image: 2022-08-24 (18).png\n","Image shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (286).png\n","Unique classes in mask: [1 2 3 4 6 8]Image shape: torch.Size([3, 1080, 1920])\n","\n","Loading image: 2022-08-24 (234).png\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (263).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (183).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (138).png\n","Unique classes in mask: [0 1 3 4 5 6 7 8]Image shape: torch.Size([3, 1080, 1920])\n","\n","Loading image: 2022-08-24 (68).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (194).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 6 7]\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Loading image: 2022-08-24 (309).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 6 8]\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (235).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (204).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 8])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]Unique classes in mask: [0 1 6 7]\n","\n","Loading image: 2022-08-24 (142).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (17).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (202).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])Unique classes in mask: [1 2 3 4 5 7 8]\n","\n","Loading image: 2022-08-24 (338).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (37).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (65).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (198).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (305).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (218).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 3 4 5 6 8]\n","Loading image: 2022-08-24 (192).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (94).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (217).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (261).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (321).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (262).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (227).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [1 2 3 4 6 7 8]\n","Loading image: 2022-08-24 (201).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (11).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (285).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (307).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (76).png\n","\n","Image shape: torch.Size([3, 1080, 1920])Unique classes in mask: [0 1 2 3 4 8]\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Loading image: 2022-08-24 (301).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (169).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (317).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (91).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6]\n","Loading image: 2022-08-24 (44).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (6).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (240).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (322).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (51).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (315).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (329).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Epoch 1/1, Loss: 0.3397\n","Training complete\n"]}]},{"cell_type":"markdown","source":["**Testing accuracy**"],"metadata":{"id":"J5sNIa7k9SG9"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","def fast_hist(a, b, n):\n","    \"\"\"Compute the histogram of a and b.\"\"\"\n","    k = (a >= 0) & (a < n)\n","    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n","\n","def compute_metrics(hist):\n","    \"\"\"Compute metrics including pixel-wise accuracy and mean IoU.\"\"\"\n","    # Pixel-wise accuracy\n","    accuracy = np.diag(hist).sum() / hist.sum()\n","    # Per-class IoU\n","    iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n","    # Mean IoU\n","    mean_iou = np.nanmean(iou)\n","    return accuracy, mean_iou\n","\n","def evaluate_model(model, data_loader, num_classes):\n","    model.eval()  # Set the model to evaluation mode\n","    device = next(model.parameters()).device\n","    hist = np.zeros((num_classes, num_classes))\n","\n","    with torch.no_grad():  # No need to track gradients\n","        for images, masks in data_loader:\n","            images, masks = images.to(device), masks.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)  # Get the predicted classes\n","\n","\n","            predicted_resized = torch.nn.functional.interpolate(predicted.unsqueeze(1).float(),\n","                                                                size=masks.size()[1:],\n","                                                                mode='nearest').squeeze(1).long()\n","\n","            # Compute the histogram of the predicted vs true labels\n","            hist += fast_hist(masks.cpu().numpy().flatten(),\n","                              predicted_resized.cpu().numpy().flatten(),\n","                              num_classes)\n","\n","    accuracy, mean_iou = compute_metrics(hist)\n","    return accuracy, mean_iou\n","\n","num_classes = 9\n","\n","# Evaluate the model\n","accuracy, mean_iou = evaluate_model(resnet18_segmentation, test_loader, num_classes)\n","print(f'Test Pixel-wise Accuracy: {accuracy:.4f}')\n","print(f'Test Mean IoU: {mean_iou:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cfj12Hff9U9v","executionInfo":{"status":"ok","timestamp":1708675197700,"user_tz":-330,"elapsed":44239,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"ca0474a1-6881-433d-fbc3-bd18099d608c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (160).png\n","Image shape: torch.Size([3, 1080, 1920])Loading image: 2022-08-24 (197).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (280).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Loading image: 2022-08-24 (310).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (111).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (124).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (179).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 5 6 8]\n","Loading image: 2022-08-24 (52).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (151).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (239).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (205).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 3 4 5 8]\n","Loading image: 2022-08-24 (237).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (29).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 3 5 6 8]\n","Loading image: 2022-08-24 (327).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (135).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (81).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [0 1 2 3 5 6 7 8]\n","Loading image: 2022-08-24 (266).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (60).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (167).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (224).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (233).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (10).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (25).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (268).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 5 7 8]\n","Loading image: 2022-08-24 (115).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 6 7 8]\n","Mask unique values: tensor([0, 1, 3, 5, 7, 8])\n","Loading image: 2022-08-24 (184).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Unique classes in mask: [1 2 3 7 8]\n","Loading image: 2022-08-24 (139).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Loading image: 2022-08-24 (264).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [1 3 4 6 8]\n","Loading image: 2022-08-24 (77).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (113).png\n","Image shape: torch.Size([3, 1080, 1920])Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","\n","Mask unique values: tensor([1, 3, 4, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]Loading image: 2022-08-24 (80).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Loading image: 2022-08-24 (189).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Unique classes in mask: [0 1 3 5 8]\n","Loading image: 2022-08-24 (324).pngLoading image: 2022-08-24 (328).png\n","\n","Image shape: torch.Size([3, 1080, 1920])Image shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([0, 1, 3, 5, 8])Mask unique values: tensor([0, 1, 6, 7])\n","\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (195).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (271).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (39).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (70).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (212).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (215).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 3 4 5 6 7]\n","Loading image: 2022-08-24 (274).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 6 8]\n","Loading image: 2022-08-24 (16).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 8])\n","Unique classes in mask: [0 1 3 4 7 8]\n","Loading image: 2022-08-24 (58).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (272).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (180).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (69).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (245).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (190).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 7]\n","Loading image: 2022-08-24 (289).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (336).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Test Pixel-wise Accuracy: 0.9344\n","Test Mean IoU: 0.2075\n"]}]}]}