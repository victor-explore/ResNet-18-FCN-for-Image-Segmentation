{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","mount_file_id":"11yXC6noorqkHPJAaUFy3SRsijJGEJhaL","authorship_tag":"ABX9TyPPSN+bJ0XLwVEF2F45Wokf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Prepare data**"],"metadata":{"id":"y6WMUp7rHbSW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rkfZPu9gqIts","executionInfo":{"status":"ok","timestamp":1708683812721,"user_tz":-330,"elapsed":3950,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"79d623d0-93e9-4098-8bf0-c116a0f0fa81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**Load data**"],"metadata":{"id":"Qt0EDKIauVx8"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","import json\n","from torch.utils.data import Dataset\n","\n","class SegmentationDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, colormap_file, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        with open(colormap_file) as f:\n","            self.colormap = json.load(f)\n","            self.color2label = {tuple(color): idx for idx, color in enumerate(self.colormap.values())}\n","        self.images = os.listdir(image_dir)\n","        print(f\"Found {len(self.images)} images.\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.images[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png'))  # Adjust if necessary\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"RGB\")\n","        mask = self.rgb_to_mask(mask)\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            mask = torch.from_numpy(mask).long()\n","\n","        # Debug statements\n","        print(f\"Loading image: {img_name}\")\n","        print(f\"Image shape: {image.shape}\")\n","        print(f\"Mask unique values: {torch.unique(mask)}\")\n","\n","        return image, mask\n","\n","    def rgb_to_mask(self, mask):\n","        \"\"\"Convert a RGB mask to a class map mask.\"\"\"\n","        mask = np.array(mask)\n","        class_map = np.zeros(mask.shape[:2], dtype=np.int32)\n","\n","        for rgb, class_id in self.color2label.items():\n","            equality = np.equal(mask, rgb)\n","            class_map[np.all(equality, axis=-1)] = class_id\n","\n","        # Debug statement\n","        print(f\"Unique classes in mask: {np.unique(class_map)}\")\n","\n","        return class_map\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","image_dir = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/images'\n","mask_dir = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/masks'\n","colormap_file = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/label2cmap.json'\n","\n","# Create the dataset\n","dataset = SegmentationDataset(image_dir, mask_dir, colormap_file, transform=transform)\n","\n","# Optionally, load and inspect a single sample for debugging\n","image, mask = dataset[0]  # Adjust the index if you want to inspect a different sample"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnLaF_1CqZx1","executionInfo":{"status":"ok","timestamp":1708683815964,"user_tz":-330,"elapsed":3247,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"ab6b7e82-0046-4ff0-ad6e-4cb49d730bdb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 298 images.\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (185).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n"]}]},{"cell_type":"markdown","source":["**Split training and testing data**"],"metadata":{"id":"G74Z9U-juZXU"}},{"cell_type":"code","source":["# Adjust the path to where you have stored 'train_test_split.json' in your Google Drive\n","split_file_path = '/content/drive/MyDrive/AIP Assignment 2/Question 2/dataset/train_test_split.json'\n","\n","# Read the JSON file\n","with open(split_file_path, 'r') as file:\n","    split_data = json.load(file)\n","\n","train_images = split_data['train']\n","test_images = split_data['test']\n","\n","print(f\"Total training images: {len(train_images)}\")\n","print(f\"Total testing images: {len(test_images)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FES0aD7evIFF","executionInfo":{"status":"ok","timestamp":1708683815965,"user_tz":-330,"elapsed":17,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"2a9e8169-7992-4e7b-ce4d-5fbc4a302cf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training images: 248\n","Total testing images: 50\n"]}]},{"cell_type":"code","source":["class SegmentationDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, colormap_file, subset, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        with open(colormap_file) as f:\n","            self.colormap = json.load(f)\n","            self.color2label = {tuple(color): idx for idx, color in enumerate(self.colormap.values())}\n","        # Use only the subset of images specified ('train' or 'test')\n","        if subset == 'train':\n","            self.images = [img for img in os.listdir(image_dir) if img in train_images]\n","        elif subset == 'test':\n","            self.images = [img for img in os.listdir(image_dir) if img in test_images]\n","        else:\n","            raise ValueError(\"Subset must be either 'train' or 'test'\")\n","        print(f\"Found {len(self.images)} {subset} images.\")\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.images[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        mask_path = os.path.join(self.mask_dir, img_name.replace('.jpg', '.png'))  # Adjust if necessary\n","        image = Image.open(img_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"RGB\")\n","        mask = self.rgb_to_mask(mask)\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","            mask = torch.from_numpy(mask).long()\n","\n","        # Debug statements\n","        print(f\"Loading image: {img_name}\")\n","        print(f\"Image shape: {image.shape}\")\n","        print(f\"Mask unique values: {torch.unique(mask)}\")\n","\n","        return image, mask\n","\n","    def rgb_to_mask(self, mask):\n","        \"\"\"Convert a RGB mask to a class map mask.\"\"\"\n","        mask = np.array(mask)\n","        class_map = np.zeros(mask.shape[:2], dtype=np.int32)\n","\n","        for rgb, class_id in self.color2label.items():\n","            equality = np.equal(mask, rgb)\n","            class_map[np.all(equality, axis=-1)] = class_id\n","\n","        # Debug statement\n","        print(f\"Unique classes in mask: {np.unique(class_map)}\")\n","\n","        return class_map"],"metadata":{"id":"RSoQCM8vvnAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the training dataset\n","train_dataset = SegmentationDataset(image_dir, mask_dir, colormap_file, 'train', transform=transform)\n","\n","# Create the testing dataset\n","test_dataset = SegmentationDataset(image_dir, mask_dir, colormap_file, 'test', transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA2Qhkgev7yK","executionInfo":{"status":"ok","timestamp":1708683816568,"user_tz":-330,"elapsed":618,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"ce382b9e-360e-4216-915b-3a7e95e4777f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 248 train images.\n","Found 50 test images.\n"]}]},{"cell_type":"markdown","source":["**Data loaders**"],"metadata":{"id":"ttEwflokwHgs"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# Set the batch size for the data loaders\n","batch_size = 5\n","\n","# Create the training data loader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# Create the testing data loader\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","# Verify the data loaders\n","print(f\"Total train batches: {len(train_loader)}\")\n","print(f\"Total test batches: {len(test_loader)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfZT2SFIwJmX","executionInfo":{"status":"ok","timestamp":1708683816569,"user_tz":-330,"elapsed":13,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"42d9be2d-8c26-4f40-c023-ecb9c52de434"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total train batches: 50\n","Total test batches: 10\n"]}]},{"cell_type":"markdown","source":["**Model - with skip connections**"],"metadata":{"id":"3pS3lXLE0cgv"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class FCNResNet18(nn.Module):\n","    def __init__(self, num_classes):\n","        super(FCNResNet18, self).__init__()\n","        # Load a pre-trained ResNet-18 model\n","        resnet18 = models.resnet18(pretrained=True)\n","\n","        # Use the features of ResNet-18 up to the final convolutional layer, excluding the average pooling and fully connected layers\n","        self.features = nn.Sequential(*list(resnet18.children())[:-2])\n","\n","        # Let's use the output after the second block (layer1 in ResNet terms), which has 64 output channels.\n","        self.skip_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n","\n","        # Final classifier that converts feature maps to predictions\n","        self.classifier = nn.Conv2d(512, num_classes, kernel_size=1)\n","\n","        # Upsampling layer to match the input image size\n","        self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False)\n","\n","    def forward(self, x):\n","        # Save the input size to upsample the final output back to this size\n","        input_size = x.size()[2:]\n","\n","        # Initial conv and bn and relu and maxpool, then through the first two blocks (layers)\n","        x_initial = self.features[0](x)\n","        x = self.features[1](x_initial)  # Layer 1 where we'll attach the skip connection\n","\n","        # Prepare the skip connection, adjusting its channels to match the number of classes\n","        skip_connection = self.skip_conv(x_initial)\n","\n","        # Continue through the rest of the ResNet features\n","        for i in range(2, len(self.features)):\n","            x = self.features[i](x)\n","\n","        # Classifier to get the class scores\n","        x = self.classifier(x)\n","\n","        # Upsample the output to match the input size\n","        x = nn.functional.interpolate(x, size=input_size, mode='bilinear', align_corners=False)\n","\n","        # Add the skip connection\n","        x += nn.functional.interpolate(skip_connection, size=input_size, mode='bilinear', align_corners=False)\n","\n","        return x"],"metadata":{"id":"CJZodaz7ej7z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training the model**"],"metadata":{"id":"uQjGM3Z7bwVE"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch import optim\n","from tqdm import tqdm\n","\n","model = FCNResNet18(num_classes=9)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Specify the loss function.\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Number of epochs\n","num_epochs = 1\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()  # Set the model to training mode\n","    running_loss = 0.0\n","    for images, true_masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","        # Move the inputs and targets to the correct device\n","        images = images.to(device)\n","        true_masks = true_masks.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(images)\n","\n","        # Calculate the loss\n","        loss = criterion(outputs, true_masks)\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    # Print average loss for the epoch\n","    epoch_loss = running_loss / len(train_loader)\n","    print(f\"Epoch: {epoch+1}, Loss: {epoch_loss:.4f}\")\n","\n","print(\"Training complete\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-dt1yJhgFXX","executionInfo":{"status":"ok","timestamp":1708684562853,"user_tz":-330,"elapsed":230840,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"feea6c64-c0d3-4c53-cbe8-0ab7f84d5ebd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:   0%|          | 0/50 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (209).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 6]\n","Loading image: 2022-08-24 (165).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (175).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 7]\n","Loading image: 2022-08-24 (191).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (305).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (59).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 5 6 8]\n","Loading image: 2022-08-24 (148).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (335).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Loading image: 2022-08-24 (281).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (225).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:   2%|▏         | 1/50 [00:11<09:02, 11.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (168).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:   4%|▍         | 2/50 [00:11<03:56,  4.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (223).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (173).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (283).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (210).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (85).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (299).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (334).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (152).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (76).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:   6%|▌         | 3/50 [00:19<04:44,  6.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 6 8]\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:   8%|▊         | 4/50 [00:19<02:57,  3.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Loading image: 2022-08-24 (321).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (56).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 2 3 6]\n","Loading image: 2022-08-24 (120).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (316).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (287).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (346).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (36).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (294).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (183).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (300).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (199).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (154).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  12%|█▏        | 6/50 [00:30<03:05,  4.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (19).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (157).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 8]\n","Loading image: 2022-08-24 (178).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (57).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (263).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (214).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (101).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (277).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (37).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  14%|█▍        | 7/50 [00:38<03:57,  5.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (112).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  16%|█▌        | 8/50 [00:39<02:45,  3.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (273).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (295).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (227).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (153).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (61).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (229).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (63).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (306).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  20%|██        | 10/50 [00:47<02:30,  3.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (242).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (133).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (219).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 6 8]\n","Loading image: 2022-08-24 (234).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (202).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (40).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (261).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 6]\n","Loading image: 2022-08-24 (341).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Unique classes in mask: [0 1 3 4 6 8]\n","Loading image: 2022-08-24 (282).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  22%|██▏       | 11/50 [00:54<02:59,  4.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 3 4 5 6 8]\n","Loading image: 2022-08-24 (192).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (71).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  24%|██▍       | 12/50 [00:55<02:20,  3.70s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (259).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (149).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (68).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (241).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (53).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 8]\n","Loading image: 2022-08-24 (348).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (238).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (41).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 5 8]Mask unique values: tensor([0, 1, 6])\n","\n","Loading image: 2022-08-24 (74).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  28%|██▊       | 14/50 [01:05<02:19,  3.87s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (138).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (126).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (337).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (290).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (291).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 5 6 8]\n","Loading image: 2022-08-24 (114).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 8])\n","Unique classes in mask: [0 1 2 3 6]\n","Loading image: 2022-08-24 (121).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (260).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (304).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (196).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 3 4 8]\n","Loading image: 2022-08-24 (13).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  32%|███▏      | 16/50 [01:15<02:21,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 3 4 5 8]\n","Loading image: 2022-08-24 (48).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Loading image: 2022-08-24 (99).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5]\n","Loading image: 2022-08-24 (176).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5])Unique classes in mask: [1 6 7]\n","\n","Loading image: 2022-08-24 (342).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (286).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7]\n","Loading image: 2022-08-24 (333).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n","Unique classes in mask: [0 1 2 3 6 7]\n","Loading image: 2022-08-24 (325).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (162).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (7).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (66).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  36%|███▌      | 18/50 [01:25<02:10,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (269).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 6 7 8]\n","Loading image: 2022-08-24 (30).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 5 6 8]\n","Loading image: 2022-08-24 (326).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (258).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (131).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (208).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (82).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 3 7 8]\n","Loading image: 2022-08-24 (344).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 7, 8])\n","Unique classes in mask: [0 1 3 5 6 7]\n","Loading image: 2022-08-24 (187).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (332).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  40%|████      | 20/50 [01:35<02:08,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (317).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (303).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (257).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (97).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (159).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (284).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (35).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (108).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (228).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  44%|████▍     | 22/50 [01:43<01:42,  3.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (24).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (203).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (182).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (243).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (105).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (244).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (54).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (89).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  46%|████▌     | 23/50 [01:51<02:13,  4.95s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 5 6 8]\n","Loading image: 2022-08-24 (119).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (22).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (240).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (270).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  48%|████▊     | 24/50 [01:54<01:57,  4.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 6 8]\n","Loading image: 2022-08-24 (235).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (330).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (172).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 5 6]\n","Loading image: 2022-08-24 (340).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (267).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  50%|█████     | 25/50 [01:58<01:47,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (21).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (207).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (198).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (315).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 5 6 7 8]\n","Loading image: 2022-08-24 (211).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  52%|█████▏    | 26/50 [02:02<01:42,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (226).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [0 1 3 5 7 8]\n","Loading image: 2022-08-24 (117).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Mask unique values: tensor([0, 1, 3, 5, 7, 8])\n","Loading image: 2022-08-24 (220).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 6]\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (164).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (181).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (127).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  54%|█████▍    | 27/50 [02:09<01:57,  5.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (5).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (293).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (94).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (62).png\n","Image shape: torch.Size([3, 1080, 1920])"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  56%|█████▌    | 28/50 [02:12<01:38,  4.47s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (47).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (296).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 6 7 8]\n","Loading image: 2022-08-24 (201).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (169).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (18).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  58%|█████▊    | 29/50 [02:17<01:35,  4.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (177).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (155).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 4 5 6 8]\n","Loading image: 2022-08-24 (90).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (93).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (107).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (170).png\n","Image shape: torch.Size([3, 1080, 1920])"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  60%|██████    | 30/50 [02:21<01:27,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 2 3 5 7 8]\n","Loading image: 2022-08-24 (265).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (309).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (204).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (125).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  62%|██████▏   | 31/50 [02:26<01:26,  4.58s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (292).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 2 3 4 5 8]\n","Mask unique values: tensor([0, 1, 6, 7])\n","Loading image: 2022-08-24 (84).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 3 4 5 6 8]\n","Unique classes in mask: [1 2 3 4 5 7 8]Loading image: 2022-08-24 (67).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (323).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (130).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 6]\n","Loading image: 2022-08-24 (88).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 6])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  64%|██████▍   | 32/50 [02:31<01:24,  4.71s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (163).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (339).png\n","Unique classes in mask: [1 2 3 4 8]\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (298).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  66%|██████▌   | 33/50 [02:33<01:08,  4.04s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (132).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (50).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (285).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (193).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (72).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (308).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  68%|██████▊   | 34/50 [02:39<01:12,  4.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (42).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (64).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (134).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (129).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (65).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  70%|███████   | 35/50 [02:44<01:11,  4.80s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (217).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (307).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (104).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Loading image: 2022-08-24 (279).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (288).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (92).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (43).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (216).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  74%|███████▍  | 37/50 [02:51<00:47,  3.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (100).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 4 5 6]\n","Loading image: 2022-08-24 (276).png\n","Image shape: torch.Size([3, 1080, 1920])Unique classes in mask: [0 1 6]\n","\n","Loading image: 2022-08-24 (147).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 4, 5, 6])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (222).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 3 4 5 6]\n","Mask unique values: tensor([0, 1, 6, 7])\n","Loading image: 2022-08-24 (236).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 5, 6])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Unique classes in mask: [0 1 2 3 4 5 7 8]Loading image: 2022-08-24 (6).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (91).pngImage shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6]\n","Unique classes in mask: [1 2 3 5 8]Loading image: 2022-08-24 (44).png\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (231).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6])\n","Mask unique values: tensor([1, 2, 3, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (28).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  76%|███████▌  | 38/50 [03:00<01:05,  5.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  78%|███████▊  | 39/50 [03:01<00:43,  3.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Loading image: 2022-08-24 (350).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (17).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (142).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (206).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (221).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 5 7 8]\n","Loading image: 2022-08-24 (230).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (338).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1]\n","Loading image: 2022-08-24 (78).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  80%|████████  | 40/50 [03:06<00:44,  4.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (136).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (55).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (347).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [0 1 5 6]Mask unique values: tensor([0, 1, 6, 7])\n","\n","Loading image: 2022-08-24 (331).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  82%|████████▏ | 41/50 [03:09<00:36,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (311).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 7 8]\n","Loading image: 2022-08-24 (32).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (161).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (98).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (34).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (49).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  84%|████████▍ | 42/50 [03:17<00:39,  4.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (322).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (218).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (144).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  86%|████████▌ | 43/50 [03:20<00:30,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7]\n","Loading image: 2022-08-24 (106).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (23).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (186).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (102).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (158).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 3 5 6 7 8]\n","Loading image: 2022-08-24 (166).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (51).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  88%|████████▊ | 44/50 [03:25<00:27,  4.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (137).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (200).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 3 4 6 8]\n","Mask unique values: tensor([0, 1, 6])\n","Loading image: 2022-08-24 (275).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 6, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  90%|█████████ | 45/50 [03:28<00:20,  4.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 8]\n","Loading image: 2022-08-24 (301).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (343).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 3 5 6 7]\n","Loading image: 2022-08-24 (118).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (329).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (262).png\n","Loading image: 2022-08-24 (213).png\n","Image shape: torch.Size([3, 1080, 1920])Image shape: torch.Size([3, 1080, 1920])\n","\n","Mask unique values: tensor([1, 6, 7])Mask unique values: tensor([1, 2, 3, 4, 8])\n","\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (349).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Loading image: 2022-08-24 (38).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  92%|█████████▏| 46/50 [03:36<00:21,  5.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (185).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (73).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/1:  94%|█████████▍| 47/50 [03:37<00:12,  4.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (297).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (26).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (232).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [0 1 2 3 5 6 7 8]\n","Loading image: 2022-08-24 (87).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (171).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Unique classes in mask: [0 1 2 3 8]\n","Loading image: 2022-08-24 (11).png\n","Loading image: 2022-08-24 (128).pngImage shape: torch.Size([3, 1080, 1920])\n","\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])Mask unique values: tensor([0, 1, 2, 3, 8])\n","\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (83).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Unique classes in mask: [1 2 3 4 8]\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Loading image: 2022-08-24 (312).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1:  98%|█████████▊| 49/50 [03:45<00:03,  3.73s/it]"]},{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (194).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 4 6 7 8]\n","Loading image: 2022-08-24 (14).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 4, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (351) - Copy.png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/1: 100%|██████████| 50/50 [03:50<00:00,  4.60s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 0.7334\n","Training complete\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["**Testing**"],"metadata":{"id":"ladJJmeUigIq"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","\n","def pixel_accuracy(output, mask):\n","    with torch.no_grad():\n","        predictions = torch.argmax(output, dim=1)\n","        correct = (predictions == mask).float()\n","        accuracy = correct.sum() / correct.numel()\n","    return accuracy.item()\n","\n","def iou(output, mask, num_classes):\n","    with torch.no_grad():\n","        predictions = torch.argmax(output, dim=1)\n","        iou_list = []\n","        for cls in range(num_classes):\n","            true_positive = ((predictions == cls) & (mask == cls)).sum().item()\n","            false_positive = ((predictions == cls) & (mask != cls)).sum().item()\n","            false_negative = ((predictions != cls) & (mask == cls)).sum().item()\n","\n","            if true_positive + false_positive + false_negative > 0:\n","                iou = true_positive / (true_positive + false_positive + false_negative)\n","            else:\n","                iou = float('nan')  # Undefined IoU for this class due to division by zero\n","            iou_list.append(iou)\n","\n","        # Filter out 'nan' values to avoid affecting the mean IoU\n","        iou_list = [x for x in iou_list if not np.isnan(x)]\n","        mean_iou = np.nanmean(iou_list) if len(iou_list) > 0 else float('nan')\n","    return mean_iou"],"metadata":{"id":"41kBiF2QihhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()  # Set the model to evaluation mode\n","total_accuracy = 0.0\n","total_iou = 0.0\n","num_batches = 0\n","\n","for images, masks in test_loader:\n","    images, masks = images.to(device), masks.to(device)\n","\n","    with torch.no_grad():\n","        output = model(images)\n","\n","    acc = pixel_accuracy(output, masks)\n","    miou = iou(output, masks, 9)\n","\n","    if not np.isnan(miou):\n","        total_accuracy += acc\n","        total_iou += miou\n","        num_batches += 1\n","\n","# Calculate mean values\n","mean_accuracy = total_accuracy / num_batches\n","mean_iou = total_iou / num_batches\n","\n","print(f\"Mean Pixel Accuracy: {mean_accuracy}\")\n","print(f\"Mean IoU: {mean_iou}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIqVwjsxip9b","executionInfo":{"status":"ok","timestamp":1708684757794,"user_tz":-330,"elapsed":54281,"user":{"displayName":"Investing Filter","userId":"00097480192575946567"}},"outputId":"0d61ffa7-81ed-477a-ba43-1ec7945e3a4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique classes in mask: [0 1 6 7]\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (160).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Loading image: 2022-08-24 (205).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 4 6 7 8]\n","Loading image: 2022-08-24 (280).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 6, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (29).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (111).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (135).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (179).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [0 1 2 3 5 6 7 8]\n","Loading image: 2022-08-24 (266).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 5, 6, 7, 8])\n","Unique classes in mask: [1 6 7]\n","Loading image: 2022-08-24 (151).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 6, 7])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (167).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (233).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (25).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (80).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 5 7 8]\n","Loading image: 2022-08-24 (115).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (197).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (139).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (310).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (77).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (124).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (239).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 5 6 8]\n","Loading image: 2022-08-24 (52).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 5, 6, 8])\n","Unique classes in mask: [1 3 4 5 8]\n","Loading image: 2022-08-24 (237).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 7 8]\n","Loading image: 2022-08-24 (224).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 7, 8])\n","Unique classes in mask: [0 1 3 5 6 8]\n","Loading image: 2022-08-24 (327).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 6 8]\n","Loading image: 2022-08-24 (10).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 8])\n","Unique classes in mask: [1 2 3 4 8]\n","Loading image: 2022-08-24 (81).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (268).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (60).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 6 7 8]\n","Loading image: 2022-08-24 (184).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 7, 8])\n","Unique classes in mask: [1 3 4 6 8]\n","Loading image: 2022-08-24 (113).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 6, 8])\n","Unique classes in mask: [1 2 3 7 8]\n","Loading image: 2022-08-24 (264).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (189).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (70).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 3 5 8]\n","Loading image: 2022-08-24 (328).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (212).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (195).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 6]\n","Loading image: 2022-08-24 (215).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (39).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [1 3 4 5 6 7]\n","Loading image: 2022-08-24 (274).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 6, 7])\n","Unique classes in mask: [0 1 3 4 7 8]\n","Loading image: 2022-08-24 (58).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 3, 4, 7, 8])\n","Unique classes in mask: [1 2 3 4 6 8]\n","Loading image: 2022-08-24 (16).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 6, 8])\n","Unique classes in mask: [1 2 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (272).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 5 6 7]\n","Loading image: 2022-08-24 (190).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 5, 6, 7])\n","Unique classes in mask: [1 3 4 5 6 7 8]\n","Loading image: 2022-08-24 (180).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 3, 4, 5, 6, 7, 8])\n","Unique classes in mask: [0 1 2 3 4 5 6 7]\n","Loading image: 2022-08-24 (289).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n","Unique classes in mask: [1 2 3 4 5 8]\n","Loading image: 2022-08-24 (69).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (336).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [1 2 3 4 5 7 8]\n","Loading image: 2022-08-24 (245).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([1, 2, 3, 4, 5, 7, 8])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (324).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Unique classes in mask: [0 1 6 7]\n","Loading image: 2022-08-24 (271).png\n","Image shape: torch.Size([3, 1080, 1920])\n","Mask unique values: tensor([0, 1, 6, 7])\n","Mean Pixel Accuracy: 0.9445583939552307\n","Mean IoU: 0.2741503115474298\n"]}]}]}